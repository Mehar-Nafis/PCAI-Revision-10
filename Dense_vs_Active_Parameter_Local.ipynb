{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643925f2",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  Dense vs Active Parameter (Local) â€” Phi-3 Mini (Dense) vs Mixtral (GPTQ MoE)\n",
    "\n",
    "This lab compares a **Dense Model** (`microsoft/phi-3-mini-4k-instruct`) and an **Active-Parameter / MoE Model** (`TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ`), both loaded **locally** through Hugging Face Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Objectives\n",
    "- Understand dense vs active parameter model architecture\n",
    "- Run reasoning, summarization, knowledge recall, creativity, and code benchmarks\n",
    "- Observe qualitative differences in model performance and response structure\n",
    "\n",
    "### âš™ï¸ Requirements\n",
    "- Python â‰¥ 3.10\n",
    "- `transformers`, `accelerate`, `bitsandbytes`, and optionally `auto-gptq`\n",
    "- GPU recommended (16GB+ for Mixtral GPTQ), CPU or MPS supported for Phi-3\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install -q transformers accelerate bitsandbytes auto-gptq\n",
    "\n",
    "import torch, time, platform\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(\"âœ… Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6540b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model IDs\n",
    "PHI_MODEL = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "MIXTRAL_MODEL = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "\n",
    "print(\"Dense model:\", PHI_MODEL)\n",
    "print(\"Active parameter (MoE) model:\", MIXTRAL_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69181f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "def set_pad_token_if_missing(tokenizer):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def make_generator(model, tokenizer, device):\n",
    "    def generate(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p, pad_token_id=tokenizer.eos_token_id)\n",
    "        elapsed = time.time() - start\n",
    "        text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        return text, elapsed\n",
    "    return generate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27c55f",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§ª Benchmarks\n",
    "\n",
    "We will evaluate both models across these tasks:\n",
    "1. **Reasoning**\n",
    "2. **Summarization**\n",
    "3. **Knowledge Recall**\n",
    "4. **Creative Writing**\n",
    "5. **Code Generation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "benchmarks = {\n",
    "    \"Reasoning\": \"A train travels 60 km at 60 km/h, then another 60 km at 120 km/h. What is the average speed for the whole trip?\",\n",
    "    \"Summarization\": \"Summarize this paragraph in two lines:\\n\\nHPE ProLiant servers deliver secure, software-defined compute with HPE OneView and HPE InfoSight.\",\n",
    "    \"Knowledge Recall\": \"Who proposed the theory of general relativity and when?\",\n",
    "    \"Creative Writing\": \"Write a short 4-line poem about artificial intelligence helping humanity.\",\n",
    "    \"Code Generation\": \"Write a Python function `factorial(n)` that returns n! using recursion, with a docstring.\"\n",
    "}\n",
    "print(\"Loaded benchmarks:\", list(benchmarks.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c350178b",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§© Phi-3 Mini (Dense Model)\n",
    "\n",
    "Phi-3 Mini is a **dense transformer** â€” all its parameters are active during every forward pass.  \n",
    "Itâ€™s efficient for small systems and ideal as a lightweight dense baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(PHI_MODEL)\n",
    "set_pad_token_if_missing(phi_tokenizer)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(PHI_MODEL, torch_dtype=torch.float16 if device.type==\"cuda\" else torch.float32, device_map=\"auto\")\n",
    "phi_gen = make_generator(phi_model, phi_tokenizer, device)\n",
    "\n",
    "phi_outputs = {}\n",
    "\n",
    "for task, prompt in benchmarks.items():\n",
    "    print(f\"\\n### Task: {task}\")\n",
    "    output, t = phi_gen(prompt, max_new_tokens=200)\n",
    "    phi_outputs[task] = output\n",
    "    print(f\"ðŸ•’ Time: {t:.2f}s\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66982f5f",
   "metadata": {},
   "source": [
    "\n",
    "## âš™ï¸ Mixtral (Active Parameter / MoE Model)\n",
    "\n",
    "Mixtral is a **Mixture of Experts (MoE)** model â€” it contains multiple subnetworks (\"experts\") and activates only a subset per token.  \n",
    "This makes it **computationally efficient** while retaining high capacity.\n",
    "\n",
    "> Note: Quantized Mixtral (GPTQ) version reduces memory usage and allows inference on smaller GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mix_tokenizer = AutoTokenizer.from_pretrained(MIXTRAL_MODEL, trust_remote_code=True)\n",
    "set_pad_token_if_missing(mix_tokenizer)\n",
    "\n",
    "mix_model = AutoModelForCausalLM.from_pretrained(MIXTRAL_MODEL, device_map=\"auto\", trust_remote_code=True)\n",
    "mix_gen = make_generator(mix_model, mix_tokenizer, device)\n",
    "\n",
    "mix_outputs = {}\n",
    "\n",
    "for task, prompt in benchmarks.items():\n",
    "    print(f\"\\n### Task: {task}\")\n",
    "    output, t = mix_gen(prompt, max_new_tokens=200)\n",
    "    mix_outputs[task] = output\n",
    "    print(f\"ðŸ•’ Time: {t:.2f}s\\n{output}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae534c19",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§¾ Observations and Notes\n",
    "\n",
    "- **Dense Model (Phi-3 Mini)**: Uses all weights every step. Smaller, consistent, but limited capacity.  \n",
    "- **Active Model (Mixtral)**: Uses expert routing, activating only a subset of parameters per token, achieving better efficiency and often higher quality.\n",
    "\n",
    "### Next steps\n",
    "- Add metrics like BLEU/ROUGE or factual accuracy comparisons.  \n",
    "- Try other models (e.g., `Llama-3-8B` vs. `Mixtral-8x7B`) for larger-scale testing.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
